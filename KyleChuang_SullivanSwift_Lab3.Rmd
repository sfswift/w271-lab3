---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271):
  Lab 3'
author: "PKyle Chuang and Sullivan Swift"
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---
```{r setup, include=FALSE}
require(knitr)
knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE,results='asis',warning = FALSE,fig.height=3)
options(xtable.comment=FALSE)
library(lmtest) 
library(ggplot2)
library(tseries)
library(fpp2)
library(forecast)
library(xtable)
```
```{r include=FALSE}
# setwd('~/Documents/Classes/W271 - Advanced Statistical Methods/f18-kchuangk/')

ts_plots = function(x) {
   par(mfrow=c(2,2),par=c(2,2,2,2))
   plot(x,main='t-Plot')
   hist(x)
   acf(x)
   pacf(x)
}

ts_resid = function(x){
  par(mar=c(2,2,2,2))
  ggtsdisplay(x) 
  d=data.frame(PhillipsPerron=pp.test(x)$p.value,
                      AugmentedDickeyFuller=adf.test(x)$p.value,
                      LjungBox=Box.test(x,type='Ljung-Box')$p.value)
  row.names(d)=c('p-Value')
  xtable(d)
}

forecast_exp_func=function(x){
  x$mean=exp(x$mean)
  x$upper=exp(x$upper)
  x$lower=exp(x$lower)
  x$x=exp(x$x)
  return(x)
}

arimatable = function(m,dfts){
  acc=accuracy(fitted(m),dfts)
  tbl=data.frame(Beta=m$coef,SE=diag(sqrt(m$var.coef)),Sigma2=m$sigma2,AIC=m$aic,BIC=BIC(m),LogLikelihood=m$loglik,ME=acc[1,1],RMSE=acc[1,2],MAE=acc[1,3])
  colnames(tbl)=c('beta','SE','Sigma2','AIC','BIC','LogLik','ME','RMSE','MAE')
  return(tbl)
}
```

# Question 1: Forecasting using a SARIMA model

Note: Custom function ts_plots() [plot time series], ts_resid() [plot residuals], forecast_exp() [exponentiate all values in forecasts objects] and arimatable() [summarizes arima table] are not included in the R pdf but is in the R-markdown file.

```{r echo=T,results='hide',message=F,fig.height=4}
df=read.csv('ECOMPCTNSA.csv')
head(df)
# exclude 2015 & 2016
dfts=ts(df$ECOMPCTNSA,start=c(1999,4),end=c(2014,4),freq=4)
ts_plots(dfts)
```

There is clearly a seaonsonal component based on the plot. Additionally, the plot seems to be potentially heteroskedastic. We will log the time series to decrease the heteroskedasticity and re-analyze the time series from the starting point.

```{r fig.height=4}
dfts_log=log(dfts)
ts_plots(dfts_log)
```

From the logged time series plots, there is a trend and seasonal component. The seasonality is likely to be quarterly from the t-plot. We will look at a box plot of the data by season.

```{r fig.height=2}
ggplot(data.frame(cycle=factor(cycle(dfts_log)),data=as.numeric(dfts_log)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot()
```

The seasonal boxplot does not show significant differences in each of the quarters. However, this is possibly due to the trend component affecting the cycles. We will detrend the series first and re-examine the quarterly plots.

First, the time series will be detrended using a first differencing and we examine the seasonality plot.

```{r fig.height=2}
tmp=diff(dfts_log,lag=1)
ggplot(data.frame(cycle=factor(cycle(tmp)),data=as.numeric(tmp)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot() + ggtitle('Quarterly Detrended Time Series Plot')
```

The detrended series shows a strong quarterly seasonality. The time series will be deseasonalized using a quarterly cycle.

```{r results='markup'}
df_ds=diff(diff(dfts_log,lag=1),lag=4)
ggplot(data.frame(cycle=factor(cycle(df_ds)),data=as.numeric(df_ds)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot() + ggtitle('Quarterly Detrended & Deseasonalized Time Series Plot')
adf.test(df_ds)
pp.test(df_ds)
```

The detrended and deseasonalized plot show the quarterly mean and variance is now similar across the quarters indicating a deseasonalized time series. Augmented Dickey-Fuller and Phillips-Perron test are performed on deseasonalized, detrended time series both rejecting the non-stationary hypothesis. With a stationary time series, we can use an ARMA model to model the detrended, deseasonalized time series. Since the there are $I(1)$ and $I(1)_4$ components in the time series, we will use the SARIMA model to model the original logged time series rather than modeling the detrended and deseasonalized logged data to combine the steps.

```{r}
ggtsdisplay(df_ds)
```

--SS Do we use the deseasonalized, detrended data to decide where to start with our SARIMA? or the original data?

The t-plot shows the time series to have been detrended and deseasonalized. On the pacf plot, there appears to strong serial correlation quarterly as it oscillates towards zero. The acf plot has high serial correlation at the 4th lag. The strong acf at lag 4 and cycling towards 0 in pact suggest there is a SMA(1) component. 

Based on our exploration, the intial model will be $SARIMA(0,1,0)(0,1,1)_4$.

```{r}
m = arima(dfts_log,order=c(0,1,0),seasonal = list(order=c(0,1,1),period=4))
xtable(arimatable(m,dfts_log))
```

The SMA1 $\beta$ at -0.5167 with se. at $0.0975$ suggesting signficance. The $\beta_{sma1}$ does not cross zero.

```{r}
ts_resid(m$residuals)
```

The t-plot of the residuals appears to be white noise with heteroskedasticity. We will not further attempt to fit the variance of the residuals with GARCH/ARCH models. The t-plot shows no trend or seasonality. The residuals appear to be a stationary white noise process (though not necessarily Gaussian white noise) from the augmented Dickey Fuller and Phillips-Perron Test. Finally, the Ljung-box test almost shows the residuals to be uncorrelated with a p-Value of 0.35 as confirmed by the visual inspection.

We will examine other $SARIMA(p,1,q)(P,1,Q)_4$ up to $p=q=P=Q=2$ to aid in choosing our final model.

```{r}
results=data.frame(matrix(ncol=9,nrow=0))
for (p in 0:2){ for (q in 0:2) { for (PS in 0:2) { for (QS in 0:2) {
        m.tmp=arima(dfts_log,order=c(p,1,q),seasonal = list(order=c(PS,1,QS),period=4))
        results=rbind(results,c(p=p,d=1, q=q,P=PS,D=1,Q=QS,AIC=AIC(m.tmp),BIC=BIC(m.tmp),Log_Likelihood=m.tmp$loglik)) } } } }
colnames(results) = c('p','d','q','P','D','Q','AIC','BIC','Log_Likelihood')
xtable(head(results[order(results$AIC,results$BIC),],5))
```
```{r echo=F}
arimatable(auto.arima(dfts_log),dfts_log)
```

Auto.arima() selected $SARIMA(0,1,0)(2,1,0)_4$. From the manual iterations and auto.arima(), $SARIMA(0,1,0)(0,1,2)_4$ and $SARIMA(0,1,0)(2,1,0)_4$ are chosen as the candidate models as they have the lowest AICs and BICs.

In the $SARIMA(0,1,0)(0,1,2)_4$, $\beta$s appear to be statistically siginficant and the residuals do appear to be white noise with stationarity and no autocorrelation. The Ljung-Box p-Value is higher than the $SARIMA(0,1,0)(0,1,1)_4$ model. Note that the heteroskedasticity in residuals no longer appear.

```{r}
m.010012=arima(dfts_log,order=c(0,1,0),seasonal = list(order=c(0,1,2),period=4))
xtable(arimatable(m.010012,dfts_log))
ts_resid(m.010012$residuals)
```

In the $SARIMA(0,1,0)(2,1,0)_4$, the $\beta$s do not include zero up to the 95% confidence interval and the residual appear to be stationary and white noise. The Ljung-Box p-Value is ???? and the heterskedasticity of the residuals is not evident.

```{r}
m.010210=arima(dfts_log,order=c(0,1,0),seasonal = list(order=c(2,1,0),period=4))
xtable(arimatable(m.010210,dfts_log))
ts_resid(m.010210$residuals)
```

We first note that these series seem comparable as a MA model can be inverted to become an AR model. An $SARIMA(0,1,0)(0,1,2)_4$ is very similar to $SARIMA(0,1,0)(2,1,0)_4$ from invertibility of MA models. 

The 2 models will be chosen as potential candidates. We will examine both in-sample and out-out-sample fits to chose the final model.

```{r fig.height=3}
actual=ts(df$ECOMPCTNSA,start=c(1999,4),freq=4)
forecast_sar=forecast_exp_func(forecast(m.010210))
forecast_sma=forecast_exp_func(forecast(m.010012))
autoplot(forecast_sar) +autolayer(exp(fitted(m.010210)),series='ARIMA(0,1,0)(2,1,0)[4]',position=position_jitter())+ ylab('ECOMPCTNSA')+autolayer(actual)
autoplot(forecast_sma) +autolayer(exp(fitted(m.010012)),series='ARIMA(0,1,0)(0,1,2)[4]',position=position_jitter())+ autolayer(actual) + ylab('ECOMPCTNSA')
```

The in-sample fits for both models are extremely close to the historical fit. The predictions for both models are extremely similar. We will select the models based on accuracy of the time series. The time series is logged to avoid overweighting the larger values on the time series due to the trend.

```{r}
pred_test=window(log(actual),start=c(2015,1))
xtable(accuracy(forecast(m.010210),pred_test),caption='$ARIMA(0,1,0)(2,1,0)_4$')
xtable(accuracy(forecast(m.010012),pred_test),caption='$ARIMA(0,1,0)(0,1,2)_4$')
```

Every accuracy measure tested showed a lower error with $SARIMA(0,1,0)(2,1,0)_4$ model. The final model chosen is
# CHECK BELOW
\[
\begin{aligned}
&SARIMA(0,1,0)(2,1,0)_4 \\
(1-0.7851B-0.23651B^2)_4(1-B)_4(1-B)x_t&=\epsilon_t \\
y_t=y_{t-1}+y_{t-4}+-0.7851y_{t-4}+-0.23651y_{t-5}+\epsilon_t
\end{aligned}
\]
```{r result='markup'}
df_full_log=ts(log(df$ECOMPCTNSA),start=c(1999,4),freq=4)
m=arima(df_full_log,order=c(0,1,0),seasonal = list(order=c(2,1,0),period=4))
xtable(arimatable(m,df_full_log))
```
The forecast for 2017 using $SARIMA(0,1,0)(2,1,0)_4$ is 
```{r}
forecast_sar=forecast_exp_func(forecast(m,h = 4))
xtable(data.frame(forecast_sar))
autoplot(forecast_sar,'Model') +autolayer(exp(fitted(m)),series='ARIMA(0,1,0)(2,1,0)[4]',position=position_jitter())+ ylab('ECOMPCTNSA')+ autolayer(exp(df_full_log),series='Actual')
```

KYLE - OWN TEST

Furthermore, the residuals appear to be heteroskedastic. From the $residuals^2$ act and pacf plots, There appears to be an AR component in the volatility suggesting a GARCH(1,0) model.

```{r}
#library(fGarch)
#m.garch=garchFit(~garch(1,0),m$residuals,trace=FALSE)
#summary(m.garch)
```

```{r}
ggtsdisplay(m$residuals^2)
```
```{r}
tmp=diff(diff(dfts_log,lag=1),lag=4)
ggtsdisplay(tmp)
ggplot(data.frame(cycle=factor(cycle(tmp)),data=as.numeric(tmp)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot()
```

```{r}
#ggplot(data.frame(r=residuals(m.garch)),aes(sample=r))+geom_qq()+geom_qq_line(col='red')
#hist(residuals(m.garch))
#predict(m.garch)
```
The series appear to be detrended and deseaonalized. From the ACF gaph

```{r fig.height=4}
par(mfrow=c(2,2))
plot(dfts_log)
hist(dfts_log)
acf(dfts_log,lag=12)
pacf(dfts_log,lag=12)
```

The initial model proposed is a $SARIMA(1,1,0)(0,0,1)_4$ model.

```{r}
m=arima(dfts_log,order=c(1,1,0),seasonal=list(order=c(0,0,1),period=4))
```

The AR(1) component is likely to be 0 given the -0.0705 $\beta$ and s.e. of 0.1347. The SMA(1) $\beta$ appears siginificant. We will examine the residuals to see if the I(1) has removed the trend.

From the t-plot, the I(1) appears to have removed the trend. The seasonality does not seem to be removed but m
```{r fig.height=7}
par(mfrow=c(2,2))
plot(m$residuals)
hist(m$residuals)
acf(m$residuals,lag=12)
pacf(m$residuals,lag=12)
plot(window(m$residuals,start=c(2008,1),end=c(2014,4)))
```

\newpage

```{r}
library(ggplot2)
```


# Question 2: Learning how to use the xts library

## Materials covered in Question 2 of this lab

  - Primarily the references listed in this document:

      - "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich. 2008. (xts.pdf)
      - "xts FAQ" by xts Development Team. 2013 (xts_faq.pdf)
      - xts_cheatsheet.pdf

# Task 1:

  1. Read 
    A. The **Introduction** section (Section 1), which only has 1 page of reading of xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    B. The first three questions in"xts FAQ"
        a. What is xts?
        b. Why should I use xts rather than zoo or another time-series package?
        c. HowdoIinstallxts?
    C. The "A quick introduction to xts and zoo objects" section in this document
        
  2. Read the "A quick introduction to xts and zoo objects" of this document

# A quick introduction to xts and zoo objects

### xts
```xts```
  - stands for eXtensible Time Series
  - is an extended zoo object
  - is essentially matrix + (time-based) index (aka, observation + time)

  - xts is a constructor or a subclass that inherits behavior from parent (zoo); in fact, it extends the popular zoo class. As such. most zoo methods work for xts
  - is a matrix objects; subsets always preserve the matrix form
  - importantly, xts are indexed by a formal time object. Therefore, the data is time-stamped
  - The two most important arguments are ```x``` for the data and ```order.by``` for the index. ```x``` must be a vector or matrix. ```order.by``` is a vector of the same length or number of rows of ```x```; it must be a proper time or date object and be in an increasing order

# Task 2:

  1. Read 
    A. Section 3.1 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
    B. The following questions in "xts FAQ"
        a. How do I create an xts index with millisecond precision?
        b. OK, so now I have my millisecond series but I still can't see the milliseconds displayed. What went wrong?

  2. Follow the following section of this document


# Creating an xts object and converting to an xts object from an imported dataset

We will create an `xts` object from a matrix and a time index. First, let's create a matrix and a time index.  The matrix, as it creates, is not associated with the time indext yet.

```{r include=FALSE}
rm(list = ls())
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```


```{r}
# Set working directory
#wd <-"~/Documents/Teach/Cal/w271/_2018.03_Fall/labs/lab3"
#setwd(wd)
```

```{r}
# Create a matrix
x <- matrix(rnorm(200), ncol=2, nrow=100)
colnames(x) <- c("Series01", "Series02")
str(x)
head(x,10)

idx <- seq(as.Date("2015/1/1"), by = "day", length.out = 100)
str(idx)
head(idx)
tail(idx)
```

In a nutshell, `xts` is a matrix indexed by a time object. To create an xts object, we "bind" the object with the index.  Since we have already created a matrix and a time index (of the same length as the number of rows of the matrix), we are ready to "bind" them together. We will name it *X*.

```{r}
library(xts)
X <- xts(x, order.by=idx)
str(X)
head(X,10)
```

As you can see from the structure of an `xts` objevct, it contains both a data component and an index, indexed by an objevct of class `Date`.

**xtx constructor**

```
xts(x=Null,
    order.by=index(x),
    frequency=NULL,
    unique=NULL,
    tzone=Sys.getenv("TZ"))
```
As mentioned previous, the two most important arguments are ```x``` and ```order.by```.  In fact, we only use these two arguments to create a xts object before.


With a xts object, one can decompose it.

### Deconstructing xts

```coredata()``` is used to extract the data component
```{r}
head(coredata(X),5)
```

```index()``` is used to extract the index (aka times)
```{r}
head(index(X),5)
```
  
### Conversion to xts from other time-series objects

We will use the same dataset "bls_unemployment.csv" that we used in the last live session to illustrate the functions below.


```{r}
df <- read.csv("bls_unemployment.csv", header=TRUE, stringsAsFactors = FALSE)

# Examine the data structure
  str(df)
  names(df)
  head(df)
  tail(df)

table(df$Series.id, useNA = "always")
table(df$Period, useNA = "always")

# Convert a column of the data frame into a time-series object
unemp <- ts(df$Value, start = c(2007,1), end = c(2017,1), frequency = 12)
  str(unemp)
  head(cbind(time(unemp), unemp),5)

# Now, let's convert it to an xts object
df_matrix <- as.matrix(df)
  head(df_matrix)
  str(df_matrix)
  rownames(df)

unemp_idx <- seq(as.Date("2007/1/1"), by = "month", length.out = 
length(df[,1]))
  head(unemp_idx)

unemp_xts <- xts(df$Value, order.by = unemp_idx)
  str(unemp_xts)
  head(unemp_xts)
```

# Task 3:

  1. Read 
    A. Section 3.2 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
  2. Follow the following section of this document
  
# Merging and modifying time series

One of the key strengths of ```xts``` is that it is easy to join data by column and row using a only few different functions. It makes creating time series datasets almost effortless.

The important criterion is that the xts objects must be of identical type (e.g. integer + integer), or be POSIXct dates vector, or be atomic vectors of the same type (e.g. numeric), or be a single NA. It does not work on data.frames with various column types.

The major functions is ```merge```.  It works like ```cbind``` or SQL's ```join```:

Let's look at an example. It assumes that you are familiar with concepts of inner join, outer join, left join, and right join.

```{r}
library(quantmod)
getSymbols("TWTR")
head(TWTR)
str(TWTR)
```

Note that the date obtained from the getSymbols function of the quantmod library is already an xts object.  As such, we can merge it directly with our unemployment rate xts object constructed above. Nevertheless, it is instructive to examine the data using the View() function to ensure that you understand the number of observations resulting from the joined series.

```{r}
# 1. Inner join
TWTR_unemp01 <- merge(unemp_xts, TWTR, join = "inner")
  str(TWTR_unemp01)
  head(TWTR_unemp01)

# 2. Outer join (filling the missing observations with 99999)
# Basic argument use
TWTR_unemp02 <- merge(unemp_xts, TWTR, join = "outer", fill = 99999)
  str(TWTR_unemp02)
  head(TWTR_unemp02)
  #View(TWTR_unemp02)

# Left join
TWTR_unemp03 <- merge(unemp_xts, TWTR, join = "left", fill = 99999)
  str(TWTR_unemp03)
  head(TWTR_unemp03)
  #View(TWTR_unemp03)
  
# Right join
TWTR_unemp04 <- merge(unemp_xts, TWTR, join = "right", fill = 99999)
  str(TWTR_unemp04)
  head(TWTR_unemp04)
  #View(TWTR_unemp04)
```

# Missing value imputation
xts also offers methods that allows filling missing values using last or previous observation. Note that I include this simply to point out that this is possible. I by no mean certify that this is the preferred method of imputing missing values in a time series.  As I mentioned in live session, the specific method to use in missing value imputation is completely context dependent.

Filling missing values from the last observation
```{r}
# First, let's replace the "99999" values with NA and then exammine the series. 

# Let's examine the first few dozen observations with NA
TWTR_unemp02['2013-10-01/2013-12-15'][,1]

# Replace observations with "99999" with NA and store in a new series
unemp01 <- TWTR_unemp02[, 1]
unemp01['2013-10-01/2013-12-15']
str(unemp01)
head(unemp01)
#TWTR_unemp02[, 1][TWTR_unemp02[, 1] >= 99990] <- NA

unemp02 <- unemp01
unemp02[unemp02 >= 99990] <- NA

cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'])

# Impute the missing values (stored as NA) with the last observation
TWTR_unemp02_v2a <- na.locf(TWTR_unemp02[,1], 
                            na.rm = TRUE, fromLast = TRUE) 
unemp03 <- unemp02
unemp03 <- na.locf(unemp03, na.rm = TRUE, fromLast = FALSE);

# Examine the pre- and post-imputed series
cbind(TWTR_unemp02['2013-10-01/2013-12-30'][,1], TWTR_unemp02_v2a['2013-10-01/2013-12-15'])

cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'],
unemp03['2013-10-01/2013-12-15'])
```

Another missing value imputation method is linear interpolation, which can also be easily done in xts objects. In the following example, we use linear interpolation to fill in the NA in between months.  The result is stored in ```unemp04```. Note in the following the different ways of imputing missing values.

```{r}
unemp04 <- unemp02
unemp04['2013-10-01/2014-02-01']
unemp04 <- na.approx(unemp04, maxgap=31)
unemp04['2013-10-01/2014-02-01']

round(cbind(unemp01['2013-10-01/2013-12-15'], unemp02['2013-10-01/2013-12-15'],
unemp03['2013-10-01/2013-12-15'],
unemp04['2013-10-01/2013-12-15']),2)
```

## Calculate difference in time series
A very common operation on time series is to take a difference of the series to transform a non-stationary series to a stationary series. First order differencing takes the form $x(t) - x(t-k)$ where $k$ denotes the number of time lags. Higher order differences are simply the reapplication of a difference to each prior result (like a second derivative or a difference of the difference).

Let's use the ```unemp_xts``` series as examples:
```{r}
str(unemp_xts)
unemp_xts
 
diff(unemp_xts, lag = 1, difference = 1, log = FALSE, na.pad = TRUE)

# calculate the first difference of AirPass using lag and subtraction
#AirPass - lag(AirPass, k = 1)

# calculate the first order 12-month difference if AirPass
diff(unemp_xts, lag = 12, differences = 1)
```

# Task 4:

  1. Read 
    A. Section 3.4 of "xts: Extensible Time Series" by Jeffrey A. Ryan and Joshua M. Ulrich
    
    B. the following questions in "xts FAQ"
        a. I am using apply() to run a custom function on my xts series. Why the returned matrix has different dimensions than the original one?

  2. Follow the following two sections of this document

# Apply various functions to time series

The family of ```apply``` functions perhaps is one of the most powerful R function families. In time series, ```xts``` provides ```period.apply```, which takes (1) a time series, (2) an index of endpoints, and (3) a function to apply.  It takes the following general form:
```
period.apply(x, INDEX, FUN, ...)
```

As an example, we use the Twitter stock price series (to be precise, the daily closing price), create an index storing the points corresopnding to the weeks of the daily series, and apply functions to calculate the weekly mean. 

```{r}
# Step 1: Identify the endpoints; in this case, we use weekly time interval. That is, we extract the end index on each week of the series

#View(TWTR)
head(TWTR)
TWTR_ep <- endpoints(TWTR[,4], on = "weeks")
#TWTR_ep

# Step 2: Calculate the weekly mean
TWTR.Close_weeklyMean <- period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = mean)
head(round(TWTR.Close_weeklyMean,2),8)
```

The power of the apply function really comes with the use of custom-defined function. For instance, we can easily 

```{r}
f <- function(x) {
  mean <- mean(x)
  quantile <- quantile(x,c(0.05,0.25,0.50,0.75,0.95))
  sd <- sd(x)
  
  result <- c(mean, sd, quantile)
  return(result)
}
head(round(period.apply(TWTR[, 4], INDEX = TWTR_ep, FUN = f),2),10)
```

# Calculate basic rolling statistics  of series by month
Using ```rollapply```, one can calculate rolling statistics of a series:

```{r}
# Calculate rolling mean over a 10-day period and print it with the original series
head(cbind(TWTR[,4], rollapply(TWTR[, 4], 10, FUN = mean, na.rm = TRUE)),15)
```

# Task 5:
1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames

```{r}
amaz <- read.csv('AMAZ.csv')
umcsent <- read.csv('UMCSENT.csv')

head(amaz)
tail(amaz)

length(amaz$Index)

head(umcsent)
tail(umcsent)

length(umcsent$Index)
```

2. Convert them to xts objects

```{r}
amaz.xts <- as.xts(amaz[,2:6],order.by=as.Date(amaz$Index,format="%Y-%m-%d"))

head(amaz.xts)
tail(amaz.xts)

umcsent.xts <- as.xts(umcsent[,2], order.by=as.Date(umcsent$Index, format="%Y-%m-%d"))

head(umcsent.xts)
tail(umcsent.xts)
```

It is important to note here that the `amaz.xts` has a shorter duration and span than the `umcscent.xts` series. However, the `amaz.xts` series has more observations than `umcsent.xts`.

3. Merge the two set of series together, perserving all of the observations in both set of series

```{r}
merged <- merge(amaz.xts, umcsent.xts, join="outer")
head(merged)
tail(merged)

length(merged)
```
    
    a. fill all of the missing values of the UMCSENT series with -9999

--SS: I'm a little confused here. Does he mean the original UMCSENT or the one we merged with?

```{r}
umcsent.na <- merge(amaz.xts, umcsent.xts, join="outer", fill = -9999)
```
    
    b. then create a new series, named UMCSENT02, from the original UMCSENT series replace all of the -9999 with NAs

--SS: I don't think the original has any -9999s?

```{r}
umcsent02 <- umcsent
umcsent02[umcsent02 <= -9999 ] <- NA
```
    
    c. then create a new series, named UMCSENT03, and replace the NAs with the last observation

```{r}
umcsent03 <- na.locf(umcsent02[,1], na.rm = TRUE, fromLast = TRUE) 
```
    
    d. then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.
    
```{r}
umcsent04 <- na.locf(umcsent02[,1], na.rm = TRUE, fromLast = FALSE)
```
    
    e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)
    
```{r}

```

4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

```{r}
f <- function(x) {
  
}
head(cbind(TWTR[,4], rollapply(merged$AMAZ.close, 10, FUN = mean, na.rm = TRUE)),15)
```

5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.

```{r}

```