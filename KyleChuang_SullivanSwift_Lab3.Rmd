---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271):
  Lab 3'
author: "Kyle Chuang and Sullivan Swift"
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---
```{r setup, include=FALSE}
require(knitr)
knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE,results='asis',warning = FALSE,fig.height=3)
options(xtable.comment=FALSE)
options(xtable.floating=FALSE)
options(xtable.NA.string='NA')
options(xtable.table.placement="!h")
library(lmtest) 
library(ggplot2)
library(tseries)
library(fpp2)
library(forecast)
library(xtable)
```
```{r include=FALSE}
ts_plots = function(x) {
   par(mfrow=c(2,2),par=c(2,2,2,2))
   plot(x,main='t-Plot')
   hist(x)
   acf(x)
   pacf(x)
}

ts_resid = function(x){
  par(mfrow=c(2,2),par=c(2,2,2,2))
  plot(x,main='t-Plot')
  qqnorm(x)
  qqline(x,col='red')
  acf(x)
  pacf(x)
  d=data.frame(PhillipsPerron=pp.test(x)$p.value,
               AugmentedDickeyFuller=adf.test(x)$p.value,
               LjungBox=Box.test(x,type='Ljung-Box')$p.value,
               ShapiroWilkes=shapiro.test(x)$p.value)
  xtable(d)
  
  ShapiroWilks <- shapiro.test(x)$p.value
}

forecast_exp_func=function(x){
  x$mean=exp(x$mean)
  x$upper=exp(x$upper)
  x$lower=exp(x$lower)
  x$x=exp(x$x)
  return(x)
}

arimatable = function(m,dfts){
  acc=accuracy(fitted(m),dfts)
  tbl=data.frame(Beta=m$coef,SE=diag(sqrt(m$var.coef)),Sigma2=m$sigma2,AIC=m$aic,BIC=BIC(m),LogLikelihood=m$loglik,ME=acc[1,1],RMSE=acc[1,2],MAE=acc[1,3])
  colnames(tbl)=c('beta','SE','Sigma2','AIC','BIC','LogLik','ME','RMSE','MAE')
  return(tbl)
}

run_arima_loop = function(x) {
  results=data.frame(matrix(ncol=9,nrow=0))
  for (p in 0:2){ for (q in 0:2) { for (PS in 0:2) { for (QS in 0:2) {
          m.tmp=arima(x,order=c(p,1,q),seasonal = list(order=c(PS,1,QS),period=4))
          results=rbind(results,c(p=p,d=1, q=q,P=PS,D=1,Q=QS,AIC=AIC(m.tmp),BIC=BIC(m.tmp),Log_Likelihood=m.tmp$loglik)) } } } }
  colnames(results) = c('p','d','q','P','D','Q','AIC','BIC','Log_Likelihood')
  return(results)
}
```

## Question 1: Forecasting using a SARIMA model

Note: Custom function ts_plots() [plot time series], ts_resid() [plot residuals], forecast_exp() [exponentiate all values in forecasts objects], arimatable() [summarizes arima table] and run_arima_loop [loop through $p$ and $q$ variables on an arima model] are not included in the R pdf but is in the R-markdown file.

In the following report, we analyze and model quarterly data of E-Commerce Retail Sales as the Percent of Total Sales. Our goal is to use the data, ranging from Q4 1999 to Q4 2016, to forecast predictions for each quarter in 2017. First, we explore the data and determine what models to further pursue. We selected two models to test in depth, including diagnostic tests and with in- and out-of-model sampling. We use our final model to make a 2017 prediction.

We begin by examining a time series plot, a histogram of the time series and the ACF and PACF plots.

```{r echo=T,results='hide',message=F,fig.height=4}
df=read.csv('ECOMPCTNSA.csv')
head(df)
# exclude 2015 & 2016
dfts=ts(df$ECOMPCTNSA,start=c(1999,4),end=c(2014,4),freq=4)
ts_plots(dfts)
```

There is clearly a seaonsonal component based on the plot. Additionally, the plot seems to be potentially heteroskedastic. We will perform a `log` transformation on the time series to decrease the heteroskedasticity and re-analyze the time series from the starting point.

```{r fig.height=4}
dfts_log=log(dfts)
ts_plots(dfts_log)
```

From the logged time series plots, there is a trend and seasonal component. The seasonality is likely to be quarterly from the t-plot. We will look at a box plot of the data by season.

```{r fig.height=2}
ggplot(data.frame(cycle=factor(cycle(dfts_log)),data=as.numeric(dfts_log)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot()
```

The seasonal boxplot does not show significant mean and variance differences in each of the quarters. However, this is possibly due to the trend component affecting the cycles. We will detrend the series by first differencing and re-examine the quarterly plots.

```{r fig.height=2}
tmp=diff(dfts_log,lag=1)
ggplot(data.frame(cycle=factor(cycle(tmp)),data=as.numeric(tmp)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot() + ggtitle('Quarterly Detrended Time Series Plot')
```

The detrended series shows a strong quarterly seasonality. The time series will be deseasonalized using a quarterly cycle.

```{r results='markup',fig.height=2}
df_ds=diff(diff(dfts_log,lag=1),lag=4)
ggplot(data.frame(cycle=factor(cycle(df_ds)),data=as.numeric(df_ds)),aes(x=cycle,y=data,group=cycle,fill=cycle))+geom_boxplot() + ggtitle('Quarterly Detrended & Deseasonalized Time Series Plot')
```

The plot of the detrended and deseasonalized data shows the quarterly mean and variance is now similar across the quarters, indicating a deseasonalized time series.

Augmented Dickey-Fuller and Phillips-Perron tests are performed on deseasonalized, detrended time series (the residuals). Both tests reject the non-stationary hypothesis. With a stationary time series, we can use an ARMA model to model the detrended, deseasonalized time series. 

```{r results='markup'}
adf.test(df_ds)
pp.test(df_ds)
```

Since the there are $I(1)$ and $I(1)_4$ components in the time series, we will use the SARIMA model to model the original log-transformed time series rather than modeling the detrended and deseasonalized residuals, thus, combining the steps.

```{r fig.height=4}
ggtsdisplay(df_ds,main='Detrended, de-seaonalized residuals')
```

Reviewing the residuals of the detrended and deseasonalized data, we can confirm the removal of the trend and seasonality. On the PACF plot, there appears to strong serial quarterly correlation as it oscillates towards zero. The ACF plot has high serial correlation at the 4th lag. The strong ACF at lag 4 and cycling towards 0 in PACF suggest there is a seasonal MA(1) [SMA1] component. 

Based on our exploration, we know our model will have a seasonal period $s=4$, that we will need differencing of $d=1, D=1$. Our intial model will be $SARIMA(0,1,0)(0,1,1)_4$.

```{r}
m = arima(dfts_log,order=c(0,1,0),seasonal=list(order=c(0,1,1),period=4))
xtable(arimatable(m,dfts_log))
```

The SMA1 $\beta$ at $-0.5167$ with $SE=0.0975$ suggest stationarity at the 95% confidence interval as it does not cross $1$. It also does not cross $0$ suggesting significance at $lag 1$.

```{r fig.height=4}
res <- ts_resid(m$residuals)
res
```

The t-plot of the residuals appears to be white noise with heteroskedasticity. We should square the residuals and check the acf and pacf plots to possibly model the variance with GARCH/ARCH model. It is beyond the scope of the lab. The t-plot shows no trend or seasonality. The residuals appear to be a stationary process from the augmented Dickey Fuller and Phillips-Perron Test. The Shapiro-Wilks tests, $p<0.05$, indicates non-normality of residuals. While this may cause inferences on the model to be invalid, we believe that the residual population distribution is normal based on Central Limit Theorem. Thus, the rejection of the $H_0$ in Shapiro-Wilks test may be potentially overlooked. Finally, the Ljung-box test with p-Value of 0.35 and visual inspection indicates uncorrelated residual, a property of white noise. From visual inspection and various test, we believe the residuals follow a Gaussian white noise process based on stationarity, normality and non-correlation.

We will examine other $SARIMA(p,1,q)(P,1,Q)_4$ up to $p=q=P=Q=2$ to aid in choosing our final model.

```{r}
results=run_arima_loop(dfts_log)
xtable(head(results[order(results$AIC,results$BIC),],5))
```

Auto.arima() selected $SARIMA(0,1,0)(2,1,0)_4$. 

```{r}
xtable(arimatable(auto.arima(dfts_log),dfts_log))
```

From the manual iterations and auto.arima(), $SARIMA(0,1,0)(0,1,2)_4$ and $SARIMA(0,1,0)(2,1,0)_4$ are chosen as the candidate models as they have the lowest AICs and BICs.

In the $SARIMA(0,1,0)(2,1,0)_4$ below, the $\beta$s do not include zero up to the 95% confidence interval and the residual appear to be stationary and white noise. The Ljung-Box p-Value is $.13$ and the heterskedasticity of the residuals is not evident. Though normality is rejected based on Shapiro-Wilks test, Central Limit Theorem is invoked. Based on visual inspections and statistical tests, we believe the residuals follow a Gaussian white noise process allowing for testing and inferences.

```{r fig.height=4}
m.010210=arima(dfts_log,order=c(0,1,0),seasonal = list(order=c(2,1,0),period=4))
xtable(arimatable(m.010210,dfts_log))
ts_resid(m.010210$residuals)
```

In the $SARIMA(0,1,0)(0,1,2)_4$ below, $\beta$s are statistically significant and the residuals do appear to be white noise with stationarity and no autocorrelation. The Ljung-Box p-Value is higher than the $SARIMA(0,1,0)(0,1,1)_4$ model. Note that the heteroskedasticity in residuals no longer appear. Here, the Shaprio-Wilks test also gives a significant p-value, $p<0.05$, indicating the distribution is non-normal, but once again, Central Limit Theorem is invoked.

```{r fig.height=4}
m.010012=arima(dfts_log,order=c(0,1,0),seasonal = list(order=c(0,1,2),period=4))
xtable(arimatable(m.010012,dfts_log))
ts_resid(m.010012$residuals)
```

We first note that these model are very similar. We note the roots of the $SARIMA(0,1,0)(0,1,2)_4$ are `r round(Mod(polyroot(c(1,-0.77,0.4))),2)` and the roots of $SARIMA(0,1,0)(2,1,0)_4$ are `r Mod(polyroot(c(1,0.8,0.25)))`. The $SARIMA(0,1,0)(0,1,2)_4$ is invertible to $SARIMA(0,1,0)(2,1,0)_4$. Both models are quite similar. 

The 2 models are chosen as potential candidates. We will examine both in-sample and out-of-sample fits to chose the final model.

```{r fig.height=3}
actual=ts(df$ECOMPCTNSA,start=c(1999,4),freq=4)
forecast_sar=forecast_exp_func(forecast(m.010210))
forecast_sma=forecast_exp_func(forecast(m.010012))
autoplot(forecast_sar) +autolayer(exp(fitted(m.010210)),series='ARIMA(0,1,0)(2,1,0)[4]',position=position_jitter())+ ylab('ECOMPCTNSA')+autolayer(actual)
autoplot(forecast_sma) +autolayer(exp(fitted(m.010012)),series='ARIMA(0,1,0)(0,1,2)[4]',position=position_jitter())+ autolayer(actual) + ylab('ECOMPCTNSA')
```

The in-sample fits for both models are extremely close to the historical fit. The predictions for both models are extremely similar. We will select the models based on accuracy of the time series. The time series is logged to avoid overweighting the larger values on the time series due to the trend and seasonality.

```{r}
pred_test=window(log(actual),start=c(2015,1))
xtable(accuracy(forecast(m.010210),pred_test),caption='$ARIMA(0,1,0)(2,1,0)_4$')
xtable(accuracy(forecast(m.010012),pred_test),caption='$ARIMA(0,1,0)(0,1,2)_4$')
```

Every accuracy measure tested showed a lower error with $SARIMA(0,1,0)(2,1,0)_4$ model. The final model chosen is
\[
\begin{aligned}
&SARIMA(0,1,0)(2,1,0)_4 \\
(1-0.80B-0.25B^2)_4(1-B^4)(1-B)x_t&=\epsilon_t \\ 
(1-0.80B-0.25B^2)_4(x_t-x_{t-1}-x_{t-4}+x_{t-5})&=\epsilon_t \\ \\
x_t-x_{t-1}-x_{t-4}+x_{t-5}&-0.80x_{t-4}+0.80x_{t-5}+0.80x_{t-8}-0.80x_{t-9} \\
&-0.25x_{t-8}+0.25x_{t-9}+0.25x_{t-12}-0.25x_{t-13}=\epsilon_t \\ \\
x_t=x_{t-1}+x_{t-4}-x_{t-5}&+0.80x_{t-4}-0.80x_{t-5}-0.80x_{t-8}+0.80x_{t-9} \\
&+0.25x_{t-8}-0.25x_{t-9}-0.25x_{t-12}i+0.25x_{t-13}+\epsilon_t \\
\end{aligned}
\]

```{r result='markup'}
df_full_log=ts(log(df$ECOMPCTNSA),start=c(1999,4),freq=4)
m=m.010210
xtable(arimatable(m,df_full_log))
```

The forecast for 2017 using $SARIMA(0,1,0)(2,1,0)_4$ is 

```{r}
m=Arima(df_full_log,model=m)  # insert the new series into the m.010210 arima model
forecast_sar=forecast_exp_func(forecast(m,h = 4))
xtable(data.frame(forecast_sar))
autoplot(forecast_sar,'Model') +autolayer(exp(fitted(m)),series='ARIMA(0,1,0)(2,1,0)[4]',position=position_jitter())+ ylab('ECOMPCTNSA')+ autolayer(exp(df_full_log),series='Actual')
```

The forecast closely follows what we would expect of the trend going forward in 2017.

## Question 2: Learning how to use the xts library

Only Task 5 is left for brevity.

# Task 5:
1. Read AMAZ.csv and UMCSENT.csv into R as R DataFrames

```{r}
library(xts)
amaz <- read.csv('AMAZ.csv')
umcsent <- read.csv('UMCSENT.csv')
xtable(head(amaz))
xtable(tail(amaz))
length(amaz$Index)
xtable(head(umcsent))
xtable(tail(umcsent))
length(umcsent$Index)
```

2. Convert them to xts objects

```{r}
amaz.xts <- as.xts(amaz[,2:6],order.by=as.Date(amaz$Index,format="%Y-%m-%d"))
xtable(head(amaz.xts))
xtable(tail(amaz.xts))
umcsent.xts <- as.xts(umcsent[,2], order.by=as.Date(umcsent$Index, format="%Y-%d-%m"))
colnames(umcsent.xts)=c('umcsent')
xtable(head(umcsent.xts))
xtable(tail(umcsent.xts))
```

It is important to note here that the `amaz.xts` has a shorter duration and span than the `umcscent.xts` series. However, the `amaz.xts` series has more observations than `umcsent.xts`.

3. Merge the two set of series together, perserving all of the observations in both set of series 

```{r}
merged <- merge(amaz.xts, umcsent.xts, join="outer")
xtable(head(merged))
xtable(tail(merged))
dim(merged)
```
    
    a. fill all of the missing values of the UMCSENT series with -9999

```{r}
umcsent01=merged
xtable(head(umcsent01))
umcsent01=na.fill(umcsent01,-9999)
xtable(head(umcsent01))
```
    
    b. then create a new series, named UMCSENT02, from the original UMCSENT series replace all of the -9999 with NAs

```{r}
umcsent02 <- umcsent01
xtable(head(umcsent02))
umcsent02[umcsent02 <= -9999 ] <- NA
xtable(head(umcsent02))
```
    
    c. then create a new series, named UMCSENT03, and replace the NAs with the last observation

```{r}
umcsent03=umcsent02
xtable(head(umcsent03))
umcsent03 <- na.locf(umcsent02, na.rm = TRUE, fromLast = TRUE) 
xtable(head(umcsent03))
```
    
    d. then create a new series, named UMCSENT04, and replace the NAs using linear interpolation.
    
```{r}
umcsent04=umcsent02
xtable(head(umcsent04))
xtable(head(umcsent04['2007-01',],15))
umcsent04=na.approx(umcsent04,maxgap=10000)
```

Note amazon has N/As in 1/1/17 and 1/2/17 because there is no data before 1/1/03 so there is nothing to interpolate.

```{r}
xtable(head(umcsent04['2007-01',],15))
```
    
    e. Print out some observations to ensure that your merge as well as the missing value imputation are done correctly. I leave it up to you to decide exactly how many observations to print; do something that makes sense. (Hint: Do not print out the entire dataset!)
    
Observations to check the merge and imputation are printed in the above sections.

4. Calculate the daily return of the Amazon closing price (AMAZ.close), where daily return is defined as $(x(t)-x(t-1))/x(t-1)$. Plot the daily return series.

```{r}
amaz.xts.close=amaz.xts$AMAZ.Close
xtable(head(amaz.xts.close))
xtable(head(diff(amaz.xts.close,lag=1,
                 differences=1,log=FALSE,na.pad=FALSE)))
xtable(head(diff(amaz.xts.close,lag=1,
                 differences=1,log=FALSE,na.pad=FALSE)/amaz.xts.close))
tmp=cbind(amaz.xts.close,diff(amaz.xts.close,lag=1,differences=1,log=FALSE,na.pad=TRUE),diff(amaz.xts.close,lag=1,differences=1,log=FALSE,na.pad=TRUE)/amaz.xts.close)
colnames(tmp)=c('Close','Chg','PctChg')
xtable(head(tmp))
```

```{r}
plot(tmp$PctChg, main="Daily Return")
```

5. Create a 20-day and a 50-day rolling mean series from the AMAZ.close series.

The numbers below look odd but it is correct. AMAZ became a penny stock. Note, AMAZ is not Amazon.

```{r}
xtable(tail(cbind(amaz.xts.close,rollapply(amaz.xts.close,20,FUN=mean,na.rm=TRUE)),15))
xtable(tail(cbind(amaz.xts.close,rollapply(amaz.xts.close,50,FUN=mean,na.rm=TRUE)),15))
```
